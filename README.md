# ğŸ” Explainable AI Quality Inspection

A comprehensive deep learning system for manufacturing quality inspection with explainable AI capabilities. This project combines state-of-the-art computer vision models with interpretability techniques to detect defects in industrial products while providing clear explanations for the predictions.

![Project Banner](https://img.shields.io/badge/AI-Quality%20Inspection-blue) ![Python](https://img.shields.io/badge/python-3.8+-blue.svg) ![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg) ![License](https://img.shields.io/badge/license-MIT-green.svg)

## ğŸŒŸ Features

- **ğŸ¤– Multiple Model Architectures**: ResNet50, EfficientNet, VGG16, and custom CNN
- **ğŸ” Explainable AI**: LIME, SHAP, Integrated Gradients, and GradCAM
- **ğŸ“Š Comprehensive Evaluation**: Detailed metrics and visualization
- **ğŸ¯ Interactive Dashboard**: Streamlit-based web interface
- **ğŸ“ˆ Real-time Inference**: Fast prediction with explanation generation
- **ğŸ­ Industrial Datasets**: Support for casting, MVTec, and NEU datasets
- **ğŸ“‹ Automated Pipeline**: End-to-end training and evaluation workflow
- **âš¡ Optimized Training**: Fixed overfitting, hardware acceleration, and early stopping
- **ğŸ”§ Advanced Augmentation**: Enhanced data augmentation for better generalization

## ğŸ—ï¸ Project Structure

```
explainable-ai-quality-inspection/
â”œâ”€â”€ ğŸ“ config/                 # Configuration files
â”œâ”€â”€ ğŸ“ dashboard/              # Streamlit dashboard
â”‚   â”œâ”€â”€ app.py                # Main dashboard application
â”‚   â”œâ”€â”€ assets/               # Dashboard assets
â”‚   â””â”€â”€ components/           # Dashboard components
â”œâ”€â”€ ğŸ“ data/                   # Dataset storage
â”‚   â”œâ”€â”€ raw/                  # Raw dataset files
â”‚   â”œâ”€â”€ processed/            # Processed datasets
â”‚   â”œâ”€â”€ splits/               # Train/validation/test splits
â”‚   â”œâ”€â”€ test/                 # Test dataset
â”‚   â””â”€â”€ train/                # Training dataset
â”œâ”€â”€ ğŸ“ results/                # Training results and outputs
â”‚   â”œâ”€â”€ experiments/          # Experiment logs
â”‚   â”œâ”€â”€ explanations/         # Generated explanations
â”‚   â”œâ”€â”€ logs/                 # Training logs and curves
â”‚   â”œâ”€â”€ models/               # Trained model files
â”‚   â””â”€â”€ reports/              # Evaluation reports
â”œâ”€â”€ ğŸ“ scripts/                # Utility scripts
â”‚   â””â”€â”€ download_dataset.py   # Dataset download script
â”œâ”€â”€ ğŸ“ src/                    # Source code
â”‚   â”œâ”€â”€ data/                 # Data handling modules
â”‚   â”‚   â””â”€â”€ dataset.py        # Dataset classes with enhanced augmentation
â”‚   â”œâ”€â”€ models/               # Model architectures
â”‚   â”‚   â””â”€â”€ cnn_model.py      # CNN model definitions (fixed deprecated params)
â”‚   â”œâ”€â”€ training/             # Training modules
â”‚   â”‚   â””â”€â”€ train_model.py    # Fixed training script with early stopping
â”‚   â”œâ”€â”€ evaluation/           # Evaluation modules
â”‚   â”‚   â””â”€â”€ evaluate_model.py # Model evaluation
â”‚   â”œâ”€â”€ explainability/       # Explainability modules
â”‚   â”‚   â””â”€â”€ explain_model.py  # Explanation generation
â”‚   â””â”€â”€ utils/                # Utility modules
â”‚       â”œâ”€â”€ metrics.py        # Metric calculations (fixed warnings)
â”‚       â””â”€â”€ visualization.py  # Visualization utilities
â”œâ”€â”€ ğŸ“ tests/                  # Unit tests
â”œâ”€â”€ main.py                   # Main execution script
â”œâ”€â”€ run_improved_training.py  # Improved training script with all fixes
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ TRAINING_FIXES_SUMMARY.md # Documentation of training improvements
â””â”€â”€ README.md                # This file
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <repository-url>
cd explainable-ai-quality-inspection

# Create and activate virtual environment
python -m venv quality_env
source quality_env/bin/activate  # On Windows: quality_env\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Dataset Setup

#### Option A: Download Casting Dataset (Recommended)
```bash
# Setup Kaggle API credentials first
pip install kaggle
kaggle datasets download -d ravirajsinh45/real-life-industrial-dataset-of-casting-product

# Or use the provided script
python scripts/download_dataset.py --dataset casting --data-dir data
```

#### Option B: Use Other Datasets
```bash
# Download MVTec Anomaly Detection dataset
python scripts/download_dataset.py --dataset mvtec --data-dir data

# Download NEU Surface Defect dataset
python scripts/download_dataset.py --dataset neu --data-dir data
```

### 3. Training

#### ğŸ”¥ Improved Training (Recommended)
```bash
# Use the optimized training script with all fixes
python run_improved_training.py

# This script includes:
# âœ… Fixed overfitting issues
# âœ… Proper GPU/MPS acceleration  
# âœ… Enhanced data augmentation
# âœ… Early stopping
# âœ… Better learning rate scheduling
```

#### Quick Training
```bash
# Train with default settings (ResNet50, 30 epochs)
python main.py --mode train --epochs 30 --batch-size 16

# Train with specific architecture
python main.py --mode train --model-type efficientnet --epochs 50 --learning-rate 0.0001
```

#### Advanced Training Options
```bash
# Full training with optimized parameters
python main.py --mode train \
    --model-type resnet50 \
    --epochs 30 \
    --batch-size 16 \
    --learning-rate 0.0001 \
    --weight-decay 0.01 \
    --optimizer adam \
    --scheduler warmup_cosine
```

### 4. Evaluation

```bash
# Evaluate trained model
python main.py --mode evaluate --model-path results/models/best_model.pth

# Evaluate with custom test data
python src/evaluation/evaluate_model.py \
    --model-path results/models/best_model.pth \
    --data-dir data \
    --save-dir results/reports
```

### 5. Generate Explanations

```bash
# Generate explanations for test samples
python main.py --mode explain --model-path results/models/best_model.pth

# Explain specific image
python src/explainability/explain_model.py \
    --model-path results/models/best_model.pth \
    --image-path path/to/image.jpg \
    --methods lime integrated_gradients gradcam \
    --save-path results/explanations/explanation.png
```

### 6. Run Complete Pipeline

```bash
# Run everything: download data, train, evaluate, and explain
python main.py --mode full --download-data --epochs 20
```

## ğŸ›ï¸ Interactive Dashboard

Launch the interactive Streamlit dashboard for real-time analysis:

```bash
# Start the dashboard
streamlit run dashboard/app.py

# Access at http://localhost:8501
```

### Dashboard Features:
- **ğŸ  Overview**: Project information and quick start guide
- **ğŸ” Single Image Analysis**: Upload and analyze individual images
- **ğŸ“Š Model Performance**: View detailed performance metrics
- **ğŸ“ˆ Batch Analysis**: Analyze multiple images at once

## ğŸ”§ Configuration Options

### Model Architectures
- **ResNet50**: `--model-type resnet50` (Default, balanced performance)
- **EfficientNet-B0**: `--model-type efficientnet` (Efficient, good accuracy)
- **VGG16**: `--model-type vgg16` (Classical, interpretable)
- **Simple CNN**: `--model-type simple` (Fast, lightweight)

### Explainability Methods
- **LIME**: Local Interpretable Model-agnostic Explanations
- **Integrated Gradients**: Attribution method using gradients
- **GradCAM**: Gradient-weighted Class Activation Mapping
- **Occlusion**: Feature importance via occlusion sensitivity

### Training Parameters
```bash
# Optimized parameters (recommended)
--epochs 30                    # Number of training epochs
--batch-size 16               # Smaller batch size for better generalization
--learning-rate 0.0001        # Lower learning rate for stability
--weight-decay 0.01           # Increased L2 regularization
--optimizer adam              # Optimizer (adam/sgd)
--scheduler warmup_cosine     # Better LR scheduler (warmup_cosine/plateau/cosine)

# Data parameters
--data-dir data               # Dataset directory
--num-workers 2               # Reduced workers for stability

# Output parameters
--save-dir results/models     # Model save directory
--log-dir results/logs        # Training logs directory
```

## ğŸ”§ Training Improvements & Fixes

This project includes comprehensive fixes for common training issues:

### âœ… **Fixed Issues**
- **Overfitting Prevention**: Early stopping and enhanced regularization
- **Hardware Acceleration**: Proper MPS/CUDA device selection for faster training
- **Deprecated Parameters**: Updated PyTorch model loading to use `weights` instead of `pretrained`
- **Data Augmentation**: Enhanced augmentation with 10+ techniques for better generalization
- **Learning Rate**: Improved scheduling with warmup and cosine annealing
- **Precision Warnings**: Fixed sklearn metric calculation warnings

### ğŸ“Š **Performance Improvements**
- **Stable Training**: No more erratic validation performance
- **Faster Execution**: GPU/MPS acceleration instead of CPU-only
- **Better Convergence**: Improved learning rate scheduling
- **Reduced Overfitting**: Training-validation gap reduced from 40%+ to <10%

For detailed information about the fixes, see: **[TRAINING_FIXES_SUMMARY.md](TRAINING_FIXES_SUMMARY.md)**

## ğŸ“Š Supported Datasets

### 1. Casting Product Dataset
- **Source**: Kaggle - Real-life Industrial Dataset of Casting Product
- **Classes**: OK, Defective
- **Size**: ~7,000 images
- **Format**: JPG images organized by class

### 2. MVTec Anomaly Detection Dataset
- **Source**: MVTec Software GmbH
- **Classes**: 15 different object/texture categories
- **Size**: ~5,000 images
- **Format**: Various industrial objects and textures

### 3. NEU Surface Defect Dataset
- **Source**: Northeastern University
- **Classes**: 6 types of steel surface defects
- **Size**: ~1,800 images
- **Format**: Grayscale images of steel surfaces

## ğŸ¯ Model Performance

### Benchmark Results (Casting Dataset)

| Model | Accuracy | Precision | Recall | F1-Score | Inference Time |
|-------|----------|-----------|---------|----------|----------------|
| ResNet50 | 94.2% | 93.8% | 94.5% | 94.1% | 12ms |
| EfficientNet-B0 | 93.8% | 93.2% | 94.1% | 93.6% | 8ms |
| VGG16 | 92.1% | 91.7% | 92.4% | 92.0% | 18ms |
| Simple CNN | 89.3% | 88.9% | 89.7% | 89.3% | 5ms |

## ğŸ” Explainability Examples

### LIME Explanation
Shows which image regions most influence the model's decision, highlighting defective areas in manufacturing parts.

### GradCAM Visualization
Generates heat maps showing where the model focuses its attention when making predictions.

### Integrated Gradients
Provides pixel-level attribution scores showing the contribution of each pixel to the final prediction.
