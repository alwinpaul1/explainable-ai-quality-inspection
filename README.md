# ğŸ” Explainable AI Quality Inspection

**Advanced industrial defect detection system with comprehensive explainable AI for casting product quality inspection using TensorFlow/Keras.**

![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg) ![TensorFlow](https://img.shields.io/badge/TensorFlow-2.13+-orange.svg) ![Lines of Code](https://img.shields.io/badge/lines_of_code-1874-brightgreen.svg)

## ğŸ¯ What This System Does

This project provides a **complete production-ready solution** for automated industrial quality inspection:

### ğŸ­ **Industrial Problem Solved**
- **Real-World Dataset**: 7,348+ high-resolution casting product images
- **Binary Classification**: Automated detection of defective vs good quality products  
- **Explainable Decisions**: Four complementary AI explanation methods
- **Production Ready**: Complete pipeline from data to deployment

### ğŸš€ **Key Capabilities**
- **ğŸ§  Advanced CNN**: Optimized Sequential architecture achieving 99.44% accuracy
- **ğŸ” Explainable AI**: LIME, SHAP, Grad-CAM, and Integrated Gradients
- **ğŸ“Š Comprehensive Analytics**: Detailed evaluation with confidence metrics
- **âš¡ Smart Pipeline**: Automated data download, training, and analysis
- **ğŸ¨ Rich Visualizations**: Publication-ready explanation dashboards

---

## ğŸ—ï¸ System Architecture

```
explainable-ai-quality-inspection/
â”œâ”€â”€ ğŸš€ main.py                     # Main CLI entry point - complete pipeline orchestration
â”œâ”€â”€ ğŸ“‹ requirements.txt            # Production dependencies (TensorFlow, LIME, SHAP, etc.)
â”œâ”€â”€ ğŸ“š CLAUDE.md                   # Development guide and best practices
â”œâ”€â”€ ğŸ“ src/                        # Modular source code (~1,874 lines)
â”‚   â”œâ”€â”€ ğŸ“Š data/
â”‚   â”‚   â””â”€â”€ dataset.py            # Smart Kaggle integration + TensorFlow data generators
â”‚   â”œâ”€â”€ ğŸ§  models/
â”‚   â”‚   â””â”€â”€ cnn_model.py          # Optimized CNN architecture (676,945 parameters)
â”‚   â”œâ”€â”€ ğŸ‹ï¸ training/
â”‚   â”‚   â””â”€â”€ train_model.py        # Advanced training pipeline with callbacks
â”‚   â”œâ”€â”€ ğŸ“ˆ evaluation/
â”‚   â”‚   â””â”€â”€ evaluate_model.py     # Comprehensive model evaluation and metrics
â”‚   â””â”€â”€ ğŸ” explainability/
â”‚       â””â”€â”€ explain_model.py      # Four explainability methods (LIME/SHAP/Grad-CAM/IG)
â”œâ”€â”€ ğŸ’¾ data/                       # Smart dataset management
â”‚   â””â”€â”€ casting_data/casting_data/ # 7,348 casting product images (auto-downloaded)
â”‚       â”œâ”€â”€ train/ (6,633 images)
â”‚       â”‚   â”œâ”€â”€ ok_front/         # Good quality products (2,875 images)
â”‚       â”‚   â””â”€â”€ def_front/        # Defective products (3,758 images)
â”‚       â””â”€â”€ test/ (715 images) 
â”‚           â”œâ”€â”€ ok_front/         # Good quality test set (262 images)
â”‚           â””â”€â”€ def_front/        # Defective test set (453 images)
â””â”€â”€ ğŸ“Š results/                   # Generated outputs and artifacts
    â”œâ”€â”€ models/                   # Trained models (.hdf5 format)
    â”œâ”€â”€ logs/                     # Training history and visualizations  
    â”œâ”€â”€ explanations/             # AI explanation dashboards
    â””â”€â”€ reports/                  # Evaluation reports and analytics
```

---

## ğŸ§  Advanced CNN Architecture

### **Model Specifications**
- **Input**: 300Ã—300 grayscale images
- **Architecture**: Sequential CNN with optimized layer design
- **Parameters**: 676,945 trainable parameters (~2.6 MB model size)
- **Performance**: 99.44% accuracy, 99.78% recall, 99.34% precision
- **Training**: Adam optimizer with binary crossentropy loss

### **Layer-by-Layer Design**
```python
Sequential([
    # Optimized Convolutional Feature Extraction
    Conv2D(32, kernel_size=3, strides=2, activation='relu'),  # 149Ã—149Ã—32
    MaxPooling2D(pool_size=2, strides=2),                     # 74Ã—74Ã—32
    
    Conv2D(16, kernel_size=3, strides=2, activation='relu'),  # 36Ã—36Ã—16  
    MaxPooling2D(pool_size=2, strides=2),                     # 18Ã—18Ã—16
    
    # Classification Head
    Flatten(),                                                 # 5,184 features
    Dense(128, activation='relu'), Dropout(0.2),              # 128 units + regularization
    Dense(64, activation='relu'), Dropout(0.2),               # 64 units + regularization
    Dense(1, activation='sigmoid')                             # Binary classification
])
```

### **Training Configuration**
- **Epochs**: 25 (with early stopping)
- **Batch Size**: 64 images
- **Steps per Epoch**: 150 (9,600 images per epoch)
- **Data Augmentation**: Rotation, shifts, brightness, flips
- **Validation Split**: 20% of training data

---

## ğŸ” Explainable AI System

### **Four Complementary Methods**

| Method | Type | Purpose | Visualization |
|--------|------|---------|---------------|
| **ğŸ¯ LIME** | Local Interpretability | Superpixel-based local explanations | Segmented regions |
| **ğŸŒ SHAP** | Global Feature Importance | Shapley value attribution | Heat maps |
| **ğŸ“¸ Grad-CAM** | Attention Mapping | Gradient-weighted activations | Attention overlays |
| **ğŸ¨ Integrated Gradients** | Pixel Attribution | Path-based feature attribution | Attribution maps |

### **Advanced Visualization Dashboard**
- **3Ã—3 Comprehensive Grid**: All methods in single view
- **CNN Architecture Diagram**: Layer-by-layer visual breakdown  
- **Prediction Confidence**: Probability distributions and metrics
- **Method Coverage**: Status indicators for each explanation technique
- **Interactive Elements**: Detailed attribution analysis

---

## ğŸ› ï¸ Installation & Setup

### **Requirements**
- Python 3.8+ 
- TensorFlow 2.13+
- 4GB+ RAM (8GB+ recommended)
- GPU support (optional, auto-detected)

### **Quick Setup with UV (Recommended)**
```bash
# 1. Clone repository
git clone https://github.com/alwinpaul1/explainable-ai-quality-inspection.git
cd explainable-ai-quality-inspection

# 2. Create virtual environment with uv
uv venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. Install dependencies
uv pip install -r requirements.txt
```

### **Alternative Setup (Standard)**
```bash
# Using standard Python venv
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## âš¡ How to Run

### **ğŸ¯ Complete Pipeline (Recommended)**
```bash
# Downloads dataset, trains model, evaluates, generates explanations
# Smart download: automatically skips if dataset already exists
python main.py --mode full --download-data --epochs 25 --batch-size 64

# Expected Results:
# âœ… 99.44% accuracy after 25 epochs (~45-60 minutes)
# ğŸ“Š Comprehensive evaluation reports  
# ğŸ” 4-method explainability analysis
```

### **ğŸš… Quick Verification Test**
```bash
# Fast 3-epoch test to verify system works
python main.py --mode full --download-data --epochs 3 --batch-size 32

# Expected: ~85% accuracy in 5-10 minutes
```

### **ğŸ”§ Individual Pipeline Components**

#### **Training Only**
```bash
python main.py --mode train --epochs 25 --batch-size 64 --steps-per-epoch 150
```

#### **Evaluation Only**
```bash
python main.py --mode evaluate --model-path results/models/cnn_casting_inspection_model.hdf5
```

#### **Explainability Only**
```bash
python main.py --mode explain --model-path results/models/cnn_casting_inspection_model.hdf5 --num-explanation-samples 10
```

### **ğŸ›ï¸ Advanced Configuration**
```bash
# Custom training parameters
python main.py --mode full \
    --download-data \
    --epochs 50 \
    --batch-size 128 \
    --image-size 300 \
    --steps-per-epoch 200 \
    --validation-steps 150

# GPU acceleration (auto-detected)
python main.py --mode full --download-data --gpu --epochs 25
```

---

## ğŸ“Š Results & Outputs


### **ğŸ“ Generated Artifacts**

#### **Models & Training**
- `results/models/cnn_casting_inspection_model.hdf5` - Trained model
- `results/logs/training_curves.png` - Training/validation curves
- `results/logs/training_history.json` - Complete training metrics

#### **Evaluation Reports**
- `results/reports/evaluation_results.txt` - Detailed performance metrics
- `results/reports/confusion_matrix.png` - Confusion matrix visualization
- `results/reports/roc_curve.png` - ROC curve analysis
- `results/reports/test_predictions.png` - 4Ã—4 prediction visualization

#### **Explainability Analysis**
- `results/explanations/explanation_sample_*.png` - 3Ã—3 explanation dashboards
- Individual method outputs with attribution analysis
- CNN architecture diagrams and method coverage reports

### **ğŸ“Š Advanced Visualizations**

#### **Training Analysis**
- **8Ã—8 Batch Grids**: Complete batch visualization with augmentation
- **Pixel-Level Analysis**: 25Ã—25 detailed pixel value inspection
- **Data Distribution**: Proportional analysis across train/validation/test
- **Training Curves**: Loss and accuracy progression with seaborn styling

#### **Evaluation Insights**
- **Misclassified Analysis**: Detailed examination of edge cases
- **Confidence Distributions**: Prediction probability analysis
- **ROC Analysis**: Threshold optimization curves
- **Class Balance**: Performance across ok_front vs def_front classes

---

## ğŸ”¬ Technical Specifications

### **ğŸ§® Dataset Intelligence**
- **Smart Download**: Automatic skip if complete dataset exists
- **Validation Checks**: Structure integrity, image counts, file formats
- **Class Balance**: Handles imbalanced dataset (ok_front: 3,137, def_front: 4,211)
- **Augmentation**: 7 transformation types for robust training

### **ğŸ” Explainability Engine**
- **LIME**: Local superpixel perturbation with 1000+ samples
- **SHAP**: Enhanced background generation with 10 diverse samples
- **Grad-CAM**: Gradient-weighted class activation mapping
- **Integrated Gradients**: Path-based attribution with 50-step integration

### **âš¡ Performance Optimizations**
- **Memory Efficient**: Batch processing for large images
- **GPU Compatible**: Automatic CUDA detection and memory growth
- **TensorFlow 2.x**: Modern gradient computation with @tf.function
- **Error Resilient**: Comprehensive exception handling and recovery

### **ğŸ“Š Evaluation Metrics**
- **Binary Classification**: Optimized for industrial defect detection
- **Threshold Analysis**: ROC-based optimal threshold selection
- **Statistical Significance**: Confidence intervals and error bounds
- **Production Metrics**: False positive/negative cost analysis

---

